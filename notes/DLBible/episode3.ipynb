{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三章 数值计算\n",
    "## 上溢和下溢\n",
    "- 当数值很接近0时，计算机会舍入为0，导致后续计算的问题，称为下溢\n",
    "- 当数值很大时，计算机会将数值近似为$\\infty$导致后续计算的问题，称为上溢\n",
    "- softmax函数：$softmax(x)_i=\\frac{exp(x_i)}{\\Sigma_{j=1}^{n}exp(x_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于梯度的优化方法\n",
    "- 代价函数、损失函数、误差函数：需要最小化或最大化的目标函数\n",
    "- 梯度下降：因为$f(x)<f(x-\\epsilon sign(f\\prime(x)))$，这表明我们需要向着函数导数符号的相反方向移动一小步以减小$f(x)$的值\n",
    "- 移动到$f\\prime(x)=0$时，我们找到了一个临界点，通常来说叫做局部极小点\n",
    "- 对于多维的函数，导数应改为偏导数，是对所有向量求导的向量值，记为$\\Delta_xf(x)$\n",
    "- 方向导数：函数f在$\\mu$单位向量方向上的偏导数，$\\frac{\\partial}{\\partial\\alpha}f(x+\\alpha\\mu)=\\mu^T\\Delta_xf(x)$\n",
    "- 最速下降的点计算：$x\\prime=x-\\epsilon\\Delta_xf(x)$，其中$\\epsilon$称为学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度之上：Jacobian和Hessian矩阵\n",
    "- Jacobian矩阵：输入和输出都是向量的函数的偏导数矩阵\n",
    "- Jacobian：$f:\\Bbb{R}^m\\to\\Bbb{R}^n, J\\in\\Bbb{R}^{m\\times n}, J_{i,j}=\\frac{\\partial}{\\partial x_j}f(x)_i$\n",
    "- 二阶导数：$\\frac{\\partial^2}{\\partial x_i\\partial x_j}f(x)$\n",
    "- 如果$\\frac{\\partial^2}{\\partial x_i\\partial x_j}f(x)=0$，移动$\\epsilon$步长时，代价函数将下降$\\epsilon f\\prime(x)$\n",
    "- 如果$\\frac{\\partial^2}{\\partial x_i\\partial x_j}f(x)<0$，移动$\\epsilon$步长时，代价函数将下降比$\\epsilon f\\prime(x)$大\n",
    "- 如果$\\frac{\\partial^2}{\\partial x_i\\partial x_j}f(x)>0$，移动$\\epsilon$步长时，代价函数将下降比$\\epsilon f\\prime(x)$小\n",
    "- Hessian矩阵：$H(f)(x)_{i,j}={\\partial^2}{\\partial x_i\\partial x_j}f(x)$\n",
    "- 交换律：${\\partial^2}{\\partial x_i\\partial x_j}f(x)={\\partial^2}{\\partial x_j\\partial x_i}f(x)$\n",
    "- 在$x_0$处作函数$f(x)$的近似二阶泰勒级数：$f(x)\\approx f(x_0)+(x-x_0)^Tg+\\frac{1}{2}(x-x_0)^TH(x-x_0)$，其中g为梯度，H为$x_0$点的Hessian矩阵\n",
    "- 学习率为$\\epsilon$时，新的点$x=x_0-\\epsilon g$，代入得：$f(x_0-\\epsilon g)\\approx f(x_0)-\\epsilon g^Tg+\\frac{1}{2}\\epsilon^2g^THg$\n",
    "- 当$g^THg$为正时，使上式下降最多的最优步长为：$\\frac{d}{d\\epsilon}(f(x_0)-\\epsilon g^Tg+\\frac{1}{2}\\epsilon^2g^THg)=0$得到$\\epsilon^*=\\frac{g^Tg}{g^THg}$\n",
    "- 仅适用梯度信息的优化算法称为一阶优化算法，使用Hessian矩阵的优化算法称为二阶优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 约束优化\n",
    "- 在$x$的某些集合$\\Bbb{S}$中寻找$f(x)$的最大值或最小值被称为约束优化\n",
    "- KKT方法：通用的约束优化的解决方案\n",
    "- 广义拉格朗日函数：$L(x,\\lambda,\\alpha)=f(x)+\\Sigma_i\\lambda_ig^{(i)}(x)+\\Sigma_j\\alpha_jh^{(j)}(x)$\n",
    "- $\\lambda$和$\\alpha$称为KKT乘子\n",
    "- $g^{(i)}(x)$和$h^{(j)}(x)$满足：$\\Bbb{S}=\\{x\\mid\\forall{i},g^{(i)}(x)=0\\ and\\ \\forall{j},h^{(j)}\\leq0\\}$\n",
    "- 然后通过求解$min_x\\ max_{\\lambda}\\ max_{\\alpha,\\alpha\\geq0}L(x,\\lambda,\\alpha)$即可求解$min_xf(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性最小二乘\n",
    "- 需找到函数$f(x)=\\frac{1}{2}||Ax-b||_2^2$的最小化值\n",
    "- 计算梯度：$\\nabla_xf(x)=A^T(Ax-b)=A^TAx-A^Tb$\n",
    "- 按照固定步长$\\epsilon$下降，容差为$\\delta$，见下面函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.99999973],\n",
       "       [ 1.99999947],\n",
       "       [ 2.9999992 ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "def matmul_chain(*args):\n",
    "    if len(args) == 0: return np.nan\n",
    "    result = args[0]\n",
    "    for x in args[1:]:\n",
    "        result = result@x\n",
    "    return x\n",
    "def gradient_decent(x, A, b, epsilon, delta):\n",
    "    while la.norm(matmul_chain(A.T, A, x)-matmul_chain(A.T, b)) > delta:\n",
    "        x -= epsilon*(matmul_chain(A.T, A, x)-matmul_chain(A.T, b))\n",
    "        #print(x)\n",
    "    return x\n",
    "\n",
    "x = np.zeros((4, 1), dtype=np.float64)\n",
    "A = np.arange(16, dtype=np.float64).reshape(4,4)\n",
    "b = np.arange(4, dtype=np.float64).reshape(4,1)\n",
    "epsilon = 0.0001\n",
    "delta = 1e-6\n",
    "gradient_decent(x, A, b, epsilon, delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
