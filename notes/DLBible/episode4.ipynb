{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四章 机器学习基础\n",
    "### 学习算法\n",
    "#### 任务T\n",
    "- 样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征的集合，通常表示成一个向量$x\\in\\Bbb{R}^n$，其中元素$x_i$是一个特征\n",
    "- 常见的机器学习任务：\n",
    "    - 分类：机器学习需要判断输入属于k类中的哪一类，也就是函数$f(x):\\Bbb{R}^n\\to\\{1,\\cdots,k\\}$，对象识别是其中的代表\n",
    "    - 输入缺失分类：对于输入向量某些维度不能提供的情况下，需要学习一组函数来处理分类问题\n",
    "    - 回归：机器学习需要对给定输入进行结果预测，即函数$f:\\Bbb{R}^n\\to\\Bbb{R}$\n",
    "    - 转录：机器学习观测一些非结构化表示的数据，并转录信息为离散的文本形式\n",
    "    - 机器翻译：输入一种语言的符号序列，转化成另一种语言的符号序列\n",
    "    - 结构化输出：将输入变成输出的向量及其他值的数据结构，例如语法分析\n",
    "    - 异常检测：机器学习在一组事件或对象中筛选出不正常或非典型的行为\n",
    "    - 合成和采样：生成一些和训练数据相似的新样本\n",
    "    - 缺失值填补：补充新样本中缺失的元素值\n",
    "    - 去噪：干净样本$x\\in\\Bbb{R}^n$经过位置损坏过程后得到损坏样本$x\\prime\\in\\Bbb{R}^n$，机器学习通过$x\\prime$预测$x$或其条件概率分布\n",
    "    - 密度估计或概率质量函数估计：通过输入样本估计样本的密度函数或概率质量函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 性能度量P\n",
    "- 对于分类、缺失输入分类和转录任务，P通常是准确率，或相应的错误率\n",
    "- 对于密度估计类的任务，P通常是输出模型在一些样本上概率对数的平均值\n",
    "- 测试集：用于评估机器学习系统性能度量P的输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 经验F\n",
    "- 无监督算法：训练含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。在深度学习中，我们通常要学习生成数据集的整个概率分布。或者说，通过样本集合学习出样本分布的规律$P(x)$\n",
    "- 监督算法：训练含有很多特征的数据集，不过数据集中的样本都有一个标签（label）或目标（target）。或者说，通过样本集合学习出结果的条件概率分布$P(y|x)$\n",
    "- 无监督学习实际上可以通过链式法则转换为监督学习：向量x的的联合分布可以表示为$P(x)=\\Pi_{i=1}^{n}P(x_i\\mid x_1,\\cdots,x_{i-1})$\n",
    "- 而实际上监督学习也可以通过无监督学习得到的联合分布求得：$P(y\\mid x)=\\frac{P(x,y)}{\\Sigma_{y\\prime}P(x,y\\prime)}$\n",
    "- 设计矩阵：用于表示数据集的矩阵，一行代表一个样本，一列代表一个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例：线性回归\n",
    "- 线性回归使用线性算法解决回归问题：向量$x\\in\\Bbb{R}^n$是输入，$y\\in\\Bbb{R}$是输出，y是x的线性函数，表示为$\\hat{y}=\\omega^Tx$，其中$\\omega\\in\\Bbb{R}^n$是参数向量\n",
    "- 参数向量$\\omega$也可以看做是每个输入特征向量影响预测值的权重\n",
    "- 测试集：假设有m个样本组成的设计矩阵用来评价模型性能，同样也有一个y向量组成的回归目标向量，这m个x和y向量组成了测试集\n",
    "- 性能度量P：测试集表示为$X^{(test)}$和$y^{(test)}$，一种P的设计方案是均方误差$MSE_{test}=\\frac{1}{m}\\Sigma_i(\\hat{y}^{(test)}-y^{(test)})_i^2$，其中$\\hat{y}^{(test)}$是测试集在模型上的预测值\n",
    "- 上式其实也是$\\hat{y}^{(test)}$向量和$y^{(test)}$向量的欧几里得距离的平方和，及预测值向量和目标向量差值的$L^2$平方范数，$\\frac{1}{m}\\mid\\mid \\hat{y}^{(test)}-y^{(test)}\\mid\\mid_2^2$\n",
    "- 训练集：训练设计矩阵$X^{(train)}$和训练目标向量$y^{(train)}$组成\n",
    "- 设计一个线性机器学习算法，为了减少$MSE_{test}$，我们需要最小化训练集上的均方误差，即$MSE_{train}$\n",
    "- 简单求出$MSE_{train}$导数为0的情况：$$\\nabla_{\\omega} MSE_{train}=0$$ $$\\implies\\nabla_{\\omega}\\frac{1}{m}\\mid\\mid\\hat{y}^{(train)}-y^{(train)}\\mid\\mid_2^2=0$$ $$\\implies \\nabla_{\\omega}\\frac{1}{m}\\mid\\mid X^{(train)}\\omega-y^{(train)}\\mid\\mid_2^2=0$$ $$\\implies\\omega=(X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}$$\n",
    "- 方程的解：$\\omega=(X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}$被称为正规方程\n",
    "- 实际应用中的线性回归应该包含截距项b：$\\hat{y}=\\omega^Tx+b$，截距b被称为仿射变换的偏置参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 容量、过拟合和欠拟合\n",
    "- 泛化：机器学习算法在未观测过的输入上表现良好的能力\n",
    "- 训练集和测试集数据通过数据集上的数据生成过程的概率分布生成\n",
    "- 独立同分布假设：每个数据集中的样本都是彼此相互独立的，且训练集和测试集是同分布的，采样自相同的分布\n",
    "- 决定机器学习算法效果优秀的因素：\n",
    "    - 降低训练误差\n",
    "    - 缩小训练误差和测试误差的差距\n",
    "- 欠拟合：机器学习模型不能再训练集上获得足够低的误差\n",
    "- 过拟合：训练误差和测试误差之间的差距太大\n",
    "- 通过调整模型的容量，控制模型是否偏向于过拟合或者欠拟合。模型的容量是指其你和各种函数的能力\n",
    "- 容量低的模型可能很难拟合训练集，容量高的模型可能会过拟合因为记住了不适用于测试集的训练集性质\n",
    "- 假设空间：机器学习算法对应的所有可能解决方案的函数集。如线性回归算法的假设空间为所有线性函数组成的集合\n",
    "- 广义线性回归（多项式模型）：$\\hat{y}=b+\\Sigma_i\\omega_ix^i$\n",
    "- 奥卡姆剃刀：在同样能够解释已知观测现象的假设中，应该挑选最简单的那一个\n",
    "- 非参数模型：机器学习模型中参数向量的个数不是有限且固定的\n",
    "- 最近邻回归：模型存储了训练集中所有的X和y，当需要为测试点分类时，模型会查询训练集中离该店最近的点，并返回相关的回归目标点y\n",
    "- 最近邻回归数学模型：$\\hat{y}=y_i$其中$i=argmin\\mid\\mid X_{i,:}-x\\mid\\mid_2^2$\n",
    "- 学成距离度量：对所有离x最近的$X_{i,:}$关联的$y_i$求平均得到的距离度量方法，该算法会在任意回归数据集上达到最小可能的训练误差\n",
    "- 贝叶斯误差：对已知真实分布$P(x,y)$的理想模型进行预测而出现的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 没有免费午餐定理\n",
    "- 没有免费午餐定理：在所有可能的数据生成分布上平均之后，每一个分类算法是未观测的点上都有相同的错误率\n",
    "- 因此机器学习研究的目标不是找一个通用学习算法或是绝对最好的学习算法。而是理解什么样的分布与人工智能获取经验的“真实世界”相关，什么样的学习算法在我们关注的数据生成分布上效果最好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化\n",
    "- 可以通过两种方式控制算法的性能，意识允许使用的函数种类，而是这些函数的数量\n",
    "- 线性回归加入权重衰减：$J(\\omega)=MSE_{train}+\\lambda\\omega^T\\omega$，$\\lambda$是一个预设值\n",
    "- 正则化项：通过对一个学习函数$f(x;\\theta)$添加一项正则化项的惩罚来均衡训练数据和正则化之间的关系（实质上是条件函数的斜率），从而调节欠拟合与过拟合的情况，上式中的正则化项为$\\omega^T\\omega$\n",
    "- 正则化：是指我们修改学习算法，使其降低泛化误差而非训练误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数和验证集\n",
    "- 超参数：用来控制算法行为的人为设定参数，如多项式回归中多项式的次数及权重衰减中的$\\lambda$\n",
    "- 验证集：讲训练数据分成两个不相交的子集，一个用于学习参数，另一个作为验证集，用于估计训练中或训练后的泛化误差，更新超参数\n",
    "- k-折交叉验证：将数据集分为k个不重合的子集。测试误差为k次计算后的平均测试误差。在第i次测试时，数据的第i个子集用于测试集，其他的数据用于训练集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 估计、偏差和方差\n",
    "#### 点估计\n",
    "- 点估计：令$\\{x_1,\\cdots,x_m\\}$是m个独立同分布的数据点，点估计或统计量是这些数据的任意函数$\\hat{\\theta}_m=g(x_1,\\cdots,x_m)$\n",
    "- 函数估计：用模型估计去近似函数$f$，或者估计$\\hat{f}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 偏差\n",
    "- 估计的偏差：$bias(\\hat{\\theta}_m)=\\Bbb{E}(\\hat{\\theta}_m)-\\theta$，$\\Bbb{E}(\\hat{\\theta}_m)$是该估计的数学期望\n",
    "- 当$bias(\\hat{\\theta}_m)=0$时，我们称该估计是无偏估计\n",
    "- 当$lim_{m\\to\\infty}bais(\\hat{\\theta}_m)=0$时，我们称该估计是渐进无偏估计，也就是$lim_{m\\to\\infty}\\Bbb{E}(\\hat{\\theta}_m)=\\theta$\n",
    "- 示例：伯努利分布\n",
    "    - 考虑一组服从均值为$\\theta$的伯努利分布的独立同分布的样本$\\{x_1,\\cdots,x_m\\}$：$P(x_i;\\theta)=\\theta^{x_i}(1-\\theta)^{(1-x_i)}$\n",
    "    - $\\theta$的常用估计量是训练样本的均值：$\\hat{\\theta}_m=\\frac{1}{m}\\Sigma_{i=1}^mx_i$\n",
    "    - 该$bias(\\hat{\\theta})=0$，因此均值的估计是无偏的\n",
    "- 示例：均值的高斯分布估计\n",
    "    - 考虑一组独立同分布的样本$\\{x_1,\\cdots,x_m\\}$服从高斯分布$P(x_i;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2}\\frac{(x_i-\\mu)^2}{\\sigma^2})$\n",
    "    - 样本均值估计：$\\hat{\\mu}_m=\\frac{1}{m}\\Sigma_{i=1}^mx_i$\n",
    "    - 因为$bias(\\hat{\\mu}_m)=0$，所以均值的估计是无偏的\n",
    "- 示例：方差的高斯分布估计\n",
    "    - 样本方差估计$\\hat{\\sigma}_m^2$：$\\hat{\\sigma}_m^2=\\frac{1}{m}\\Sigma_{i=1}^m(x_i-\\hat{\\mu}_m)^2$\n",
    "    - 该估计是有偏估计：$bias(\\hat{\\sigma}_m^2)=\\Bbb{E}[\\hat{\\sigma}_m^2]-\\sigma^2=\\frac{m-1}{m}\\sigma^2-\\sigma^2=-\\frac{\\sigma^2}{m}$\n",
    "    - 样本方差估计：$\\tilde{\\sigma}_m^2$：$\\tilde{\\sigma}_m^2=\\frac{1}{m-1}\\Sigma_{i=1}^m(x_i-\\tilde{\\mu}_m)^2$\n",
    "    - 该估计是无偏估计：$bias(\\tilde{\\sigma}_m^2)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方差和标准差\n",
    "- 估计的方差：$Var(\\hat{\\theta})$，就是训练集的方差\n",
    "- 估计的标准差：$SE(\\hat{\\theta})=\\sqrt{Var(\\hat{\\theta})}$\n",
    "- 均值的标准差：$SE(\\hat{\\mu}_m)=\\sqrt{Var[\\frac{1}{m}\\Sigma_{i=1}^m x_i]}=\\frac{\\sigma}{\\sqrt{m}}$\n",
    "- 以均值$\\hat{\\mu}_m$为中心的95%置信区间是：$(\\hat{\\mu}_m-1.96SE(\\hat{\\mu}_m),\\hat{\\mu}_m+1.96SE(\\hat{\\mu}_m))$\n",
    "- 机器学习中，我们通常说A算法比B算法好，是指A算法的置信区间上界小于B算法的置信区间的下界\n",
    "- 示例：伯努利分布\n",
    "    - 均值的方差估计：$Var(\\hat{\\theta}_m)=\\frac{1}{m}\\theta(1-\\theta)$\n",
    "    - 均值估计的方差下降速率是关于样本数m的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 权衡偏差和方差以最小化均方误差\n",
    "- 偏差度量偏离真实函数或参数的误差期望，方差度量数据上任意特定采样可能导致的估计期望的偏差\n",
    "- 均方误差：$MSE=\\Bbb{E}[(\\hat{\\theta}_m-\\theta)^2]=bias(\\hat{\\theta}_m)^2+Var(\\hat{\\theta}_m)$\n",
    "- 均方误差包含着偏差和方差的和，因此理想的估计是具有较小的均方误差\n",
    "- MSE与机器学习的容量、欠拟合和过拟合的概念紧密相联。增加容量会增加方差，降低偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一致性\n",
    "- 一致性条件：$plim_{m\\to\\infty}\\hat{\\theta}_m=\\theta$\n",
    "- $plim$表示依概率手链，即对于任意的$\\epsilon>0$，当$m\\to\\infty$时，有$P(\\mid\\hat{\\theta}_m-\\theta\\mid>\\epsilon)\\to 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最大似然估计\n",
    "- 考虑一组由m个样本组成的数据集$\\Bbb{X}=\\{x_1,\\cdots,x_m\\}$，独立的由未知的真实数据分布$P_{data}(x)$生成\n",
    "- 最大似然估计：$\\theta_{ML}=argmax_{\\theta}P_{model}(\\Bbb{X};\\theta)=argmax_{\\theta}\\Pi_{i=1}^m P_{model}(x_i;\\theta)$\n",
    "- 转换为求和方式方便计算：$\\theta_{ML}=argmax_{\\theta}\\Sigma_{i=1}^m logP_{model}(x_i;\\theta)$\n",
    "- 使用训练数据经验分布$\\hat{P}_{data}$相关的期望进行计算：$\\theta_{ML}=argmax_{\\theta}\\Bbb{E}_{x\\sim\\hat{P}_{data}}logP_{model}(x;\\theta)$\n",
    "- 通过KL散度度量经验分布$\\hat{P}_{data}$和模型分布$P_{model}$之间的差异：$D_{KL}(\\hat{P}_{data}\\mid\\mid P_{model})=\\Bbb{E}_{x\\sim\\hat{P}_{data}}[log\\hat{P}_{data}(x)-logP_{model}(x)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 条件对数似然和均方误差\n",
    "- 条件对数似然：$\\theta_{ML}=argmax_{\\theta}P(Y\\mid X;\\theta)$，如果样本是独立同分布的，则$\\theta_{ML}=argmax\\Sigma_{i=1}^{m}logP(y_i\\mid x_i;\\theta)$\n",
    "- 示例：线性回归作为最大似然，定义$P(y\\mid x)=\\cal{N}(y;\\hat{y}(x;\\omega),\\sigma^2)$，条件对数似然为$$\\Sigma_{i=1}^{m}log(P(y_i\\mid x_i;\\theta)=-mlog\\sigma-\\frac{m}{2}log(2\\pi)-\\Sigma_{i=1}^{m}\\frac{\\mid\\mid \\hat{y}_i-y_i\\mid\\mid^2}{2\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最大似然的性质\n",
    "- 最大似然估计在如下条件满足时具有一致性，即训练样本数趋向于无穷大时，会收敛到参数的真实值\n",
    "    - 真实分布$P_{data}$必须在模型族$P_{model}$中，否则没有估计能够还原真实分布\n",
    "    - 真实分布$P_{data}$必须刚好对应一个$\\theta$值，否则最大似然估计还原了真实分布后，也无法决定生成过程使用哪一个$\\theta$值\n",
    "- 最大似然通常是机器学习中的首选估计，当样本量过小发生过拟合时，通常采用正则化策略如权重衰减来获得方差较小的最大似然估计有偏版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯统计\n",
    "- 最大似然估计认为训练数据是随机的，而参数$\\theta$是确定但未知并且唯一的。\n",
    "- 贝叶斯统计与之相反，认为训练数据是确定的，而参数是随机且不唯一的，每个参数都有一定相应的概率，代表对参数的确定性\n",
    "- 在观察到数据前，将$\\theta$的已知知识称为先验概率分布$p(\\theta)$\n",
    "- 现在有一组数据样本${x_1,\\cdots,x_m}$，根据先验的数据似然及先验代入贝叶斯规则，得到：$$p(\\theta\\mid x_1,\\cdots,x_m)=\\frac{p(x_1,\\cdots,x_m\\mid \\theta)p(\\theta)}{p(x_1,\\cdots,x_m)}$$\n",
    "- 贝叶斯常用情景下，先验开始是相对均匀的分布或高熵的高斯分布，观测数据通常会使后验的熵下降，并集中在几个可能性很高的值\n",
    "- 贝叶斯统计与最大似然估计的区别：\n",
    "    - 最大似然估计使用$\\theta$的点估计，贝叶斯统计使用$\\theta$的全分布。如观察完m个样本后，第m+1的样本的预测分布为：$$p(x_{m+1}\\mid x_1,\\cdots,x_m)=\\int p(x_{m+1}\\mid\\theta)p(\\theta\\mid x_1,\\cdots,x_m)d\\theta$$\n",
    "    - 贝叶斯统计中的先验会影响概率质量密度朝参数空间中偏好先验的区域偏移，当训练数据有限时，贝叶斯方法通常泛化得更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最大后验（MAP）估计\n",
    "- 大多数涉及贝叶斯后验的计算是非常棘手的，点估计提供了一个可行的近似解\n",
    "- 可以让先验影响点估计的选择来利用贝叶斯方法的优点，一种合理的方式是选择最大后验（MAP）点估计：$$\\theta_{MAP}=argmax_{\\theta}p(x\\mid \\theta)=argmax_{\\theta}logp(x\\mid\\theta)+logp(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 监督学习算法\n",
    "#### 概率监督学习\n",
    "- 线性回归对应于分布族：$p(y\\mid x;\\theta)=\\cal{N}(y;\\theta^{T}x,I)$\n",
    "- 使用logistic sigmoid将线性函数的输出压缩进区间(0, 1)。解释为概率：$p(y=1\\mid x;\\theta)=\\sigma(\\theta^{T}x)$\n",
    "- 这个方法称为逻辑回归logistic regression，但实际上该模型用于分类而非回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 支持向量机（SVM）\n",
    "- SVM同样基于线性函数$w^{T}x+b$，不同于逻辑回归，SVM不输出概率，只输出类别\n",
    "- 核技巧：将线性函数重写为$w^{T}x+b=b+\\Sigma_{i=1}^{m}\\alpha_{i}x^{T}x_i$，$x_i$为训练样本，$\\alpha$是系数向量\n",
    "- 核函数：$k(x,x_i)=\\phi(x)\\cdot\\phi(x_i)$\n",
    "- 用核估计替换点积后：$f(x)=b+\\Sigma_i\\alpha_i k(x,x_i)$\n",
    "- 核函数等同于用特征函数$\\phi(x)$预处理所有输入，然后在新的转换空间学习线性模型\n",
    "- 高斯核：$k(u,v)=\\cal{N}(u-v;0,\\sigma^2I)$，其中$\\cal{N}(x;\\mu,\\Sigma)$是标准正态密度，也被称为径向基函数\n",
    "- 高斯核实际上是在执行一种模板匹配，训练标签y相关的训练样本x变成了类别y的模板\n",
    "- 使用核技巧的算法类别被称为核机器或核方法，核机器的主要缺点是计算决策函数的成本与训练样本数量线性增长\n",
    "- 支持向量机SVM通过学习主要包含零的向量$\\alpha$，缓和这个缺点，判断新样本的类别仅需要计算非零$\\alpha_i$对应的训练样本的核函数，这些训练样本称为支持向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 其他简单的监督学习算法\n",
    "- K最近邻（KNN）：在训练数据X上找到输入x的K个最近邻，然后返回训练集上对应的y的平均值\n",
    "- 决策树（DT）：将输入空间分为多个区域，每个叶节点对应一个区域，区域中的所有输入都归结为叶节点所对应权重值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 无监督学习算法\n",
    "- 将数据集表示为简单方式的方式：低维表示、稀疏表示和独立表示\n",
    "- 低维表示将数据压缩到维度更少的空间中\n",
    "- 稀疏表示将数据扩展到更多维度，但数据倾向于表示在坐标轴上\n",
    "- 独立表示将数据拆分到能独立统计的维度上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 主成分分析（PCA）\n",
    "- PCA将输入x投影表示成z，比原始输入维数更低的表示，同时也是一种元素之间彼此没有线性相关的表示\n",
    "- 假设有一个$m\\times n$的设计矩阵X，数据的均值为零，即$\\Bbb{E}[x]=0$\n",
    "- X对应的无偏样本协方差矩阵：$Var[x]=\\frac{1}{m-1}X^{T}X$，PCA是通过线性变换找到一个$Var[z]$是对角矩阵的表示$z=W^{T}x$\n",
    "- 设计矩阵X的主成分可以通过奇异值分解（SVD）得到，也就是说主成分是X的右奇异向量\n",
    "- 假设W是$X=U\\Sigma W^{T}$奇异值分解的右奇异向量，我们得到原来的特征向量方程：$X^{T}X=(U\\Sigma W^{T})^{T}U\\Sigma W^{T}=W\\Sigma^{T}U^{T}U\\Sigma W^{T}=W\\Sigma^2W^{T}$，因为根据奇异值的定义$U^{T}U=I$\n",
    "- 因此X的方差可以表示为：$Var[x]=\\frac{1}{m-1}X^TX=\\frac{1}{m-1}W\\Sigma^2W^T$\n",
    "- 所以z的协方差满足：$Var[z]=\\frac{1}{m-1}Z^TZ=\\frac{1}{m-1}W^TX^TXW=\\frac{1}{m-1}W^TW\\Sigma^2W^TW=\\frac{1}{m-1}\\Sigma^2$，因为根据奇异值定义$W^TW=i$，z的协方差是对角的，z中的元素是彼此无关的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-均值聚类（k-means）\n",
    "- k-均值聚类算法将训练集分成k个彼此靠近的不同样本聚类，可以认为该算法提供了k-维的one-hot编码向量h来表示x\n",
    "- 算法步骤：初始化k个不同的中心点$\\{\\mu_1,\\cdots,\\mu_k\\}$，然后迭代交换以下两个步骤直至收敛\n",
    "    - 步骤一：每个训练样本分配到最近的中心点$\\mu_i$所代表的聚类i\n",
    "    - 步骤二：每一个中心点$\\mu_i$更新为聚类i中所有训练样本$x_j$的均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机梯度下降（SGD）\n",
    "- 随机梯度下降的核心思想是，梯度是期望，期望可以通过小规模的样本近似估计\n",
    "- 在算法的每一步，我们从训练集中均匀抽出一小批量样本$\\Bbb{B}=\\{x_1,\\cdots,x_{m\\prime}\\}$\n",
    "- 梯度的估计可以表示成：$g=\\frac{1}{m\\prime}\\nabla_{\\theta}\\Sigma_{i=1}^{m\\prime}L(x_i,y_i,\\theta)$\n",
    "- 随后使用的梯度下降估计：$\\theta\\leftarrow\\theta-\\epsilon g$，$\\epsilon$为学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建机器学习算法\n",
    "- 机器学习算法配方：特定的数据集、代价函数、优化过程和模型\n",
    "- 如，线性回归算法的配方：\n",
    "    - X和y构成的数据集\n",
    "    - 代价函数：$J(w,b)=-\\Bbb{E}_{x,y ~ \\hat{p}_{data}}logp_{model}(y\\mid x)$\n",
    "    - 模型：$p_{model}(y\\mid x)=\\cal{N}(y; x^{T}w+b, 1)$\n",
    "    - 优化算法：求解代价函数梯度为零的正规方程\n",
    "- 最常见的代价函数的负对数似然，最小化代价函数会获得最大似然估计\n",
    "- 代价函数也可能含有附加项，如正则项，如可以将权重衰减添加到线性回归的代价函数中：$J(w,b)=\\lambda\\mid\\mid w\\mid\\mid_2^2-\\Bbb{E}_{x,y ~ \\hat{p}_{data}}logp_{model}(y\\mid x)$\n",
    "- 无监督学习时，配方需要的就是只含有X的数据集、代价函数、优化过程和模型\n",
    "- 如PCA的第一个主向量获得的代价函数为：$J(w)=\\Bbb{E}_{x ~ \\hat{p}_{data}}\\mid\\mid x-r(x;w)\\mid\\mid_2^2$，其中$r(x;w)=w^{T}xw$，且w具有范数为1\n",
    "- 有些算法，如决策树和k-均值聚类，需要使用特殊的优化算法，因为代价函数具有平坦的区域，使其无法使用梯度下降方式进行优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 促使深度学习的挑战\n",
    "- 上述简单的机器学习算法在一些人工智能问题（如语音识别或对象识别）上泛化能力不足\n",
    "- 传统机器学习算法不适合学习高维空间的复杂函数，深度学习旨在克服这一缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 维数灾难\n",
    "- 当数据的维数很高时，很多机器学习问题变得非常困难\n",
    "- 当数据的维数产生的可能分布远大于样本数时，会发现新的样本所在的分布可能没有任何训练样本，因此无法分类（获取训练样本最多的分类）或回归（获取训练样本的均值），传统机器学习通常假定使用距离最近样本的输出作为新样本的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 局部不变性和平滑正则化\n",
    "- 传统机器学习中最广泛使用的先验是平滑先验和局部不变性先验\n",
    "- 使用上述先验的缺点是，学习的函数永远不可能复杂到能划分出比训练样本数还多的空间区域，因为上述先验默认在空间点附近，输出结果不会产品可见性变化，即：$f(x)\\approx f(x+\\epsilon)$\n",
    "- 一些机器学习算法可能会提出更强的针对性的假设，如周期性，而这些假设往往大大降低了该算法的泛化能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 流形学习\n",
    "- 流形学习算法提供一个假设：认为$\\Bbb{R}^n$中大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流形中\n",
    "    - 现实生活中图像、文本、声音的概率分布都是高度集中的\n",
    "    - 我们至少能够非正式地想象这些领域和变换，即每个样本被其他高度相似的样本包围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
